<!doctype html>
<html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width, initial-scale=1.0"><meta name=description content="Gaël Varoquaux, computer / data / brain science"><link rel=alternate href=http://gael-varoquaux.info/feeds/all.atom.xml type=application/atom+xml title="Gaël Varoquaux Full Atom Feed"><title>PCA and ICA: Identifying combinations of variables -- Gaël Varoquaux: computer / data / brain science</title><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/pure/0.3.0/pure-min.css><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.2.0/css/font-awesome.min.css><link rel=stylesheet href=../theme/css/pure.css><script src=//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js></script></head><body><div class=pure-g-r id=layout><div class="sidebar sidebar-article pure-u"><header class=header-article><hgroup><a href=..><img class=avatar alt="Gaël Varoquaux" src=http://gael-varoquaux.info/images/gael.png></a><a href=.. class=article-info><h2 class=article-info>Gaël Varoquaux</h2></a><p>Fri 05 February 2010</p><a href=/>&larr;Home</a></hgroup></header></div><div class=pure-u><div class=content><section class=post><header class=post-header><h1>PCA and ICA: Identifying combinations of variables</h1><p class=post-meta> under <a class=post-category href=../tag/machine-learning.html>machine learning</a><a class=post-category href=../tag/scientific-computing.html>scientific computing</a><a class=post-category href=../tag/selected.html>selected</a><span class=social_links><a href=http://twitter.com/share class=twitter-share-button data-count=horizontal data-via=GaelVaroquaux>Tweet</a><script type=text/javascript src=http://platform.twitter.com/widgets.js async defer></script><span class=g-plusone data-size=medium></span></span></p></header></section><div class=topic><p class="topic-title first"><strong>Dimension reduction and interpretability</strong></p><p>Suppose you have statistical data that too many dimensions, in other words too many variables of the same random process, that has been observed many times. You want to find out, from all these variables (or all these dimensions when speaking in terms of multivariate data), what are the relevant combinations, or directions.</p></div><div class=section id=dimension-reduction-with-pca><h2>Dimension reduction with PCA</h2><p>If we have three-dimensional data, for instance simultaneous measurements made by three thermometers positioned at different locations in a room. The data forms a cluster of points in a 3D space:</p><img alt class=align-center src=../science/attachments/ica_pca/3d_data.jpg style="width: 50%;"><p>If the temperature in that room is conditioned by only two parameters, the setting of a heater and the outside temperature, we probably have too much data: the three sets of measurements can be expressed as a linear combination of two fluctuating variable, and an additional much smaller noise parameter. In other words, the data mostly lies in a 2D plane embedded in the 3D measurement space.</p><p>We can use PCA (Principal Component Analysis) to find this plane: PCA will give us the orthogonal basis in which the covariance matrix of our data is diagonal. The vectors of this basis point in successive orthogonal directions in which the data variance is maximum. In the case of data mainly residing on a 2D plane, the variance is much greater along the two first vectors, which define our plane of interest, than along the third one:</p><div class="figure align-center"><img alt src=../science/attachments/ica_pca/3d_data_pca_axis.jpg style="width: 50%;"><p class=caption>The covariance eigenvectors identified by PCA are shown in red. The plane defined by the 2 largest eigenvectors is shown in light red.</p></div><p>If we look at the data in the plane identified by PCA, it is clear that it was mostly 2D:</p><img alt class=align-center src=../science/attachments/ica_pca/3d_data_pca.jpg style="width: 50%;"></div><div class=section id=understanding-pca-with-a-gaussian-model><h2>Understanding PCA with a Gaussian model</h2><p>Let <cite>x</cite> and <cite>y</cite> be two normal-distributed variables, describing the processes we are observing:</p><div class=formula><i>x</i> = <span class=scriptfont>N</span>(0, 1) </div><p>and</p><div class=formula><i>y</i> = <span class=scriptfont>N</span>(0, 1) </div><p>Let <cite>a</cite> and <cite>b</cite> be two observation variables, linear combinations of <cite>x</cite> and <cite>y</cite>:</p><div class=formula><i>a</i> = <i>x</i> + <i>y</i></div><p>and</p><div class=formula><i>b</i> = 2 <i>y</i></div><p>PCA is performed by applying an SVD (singular value decomposition) on the observed data matrix:</p><div class=formula><i>Y</i> = [<i>a</i><sub>1</sub><i>a</i><sub>2</sub><i>a</i><sub>3</sub>...; <i>b</i><sub>1</sub><i>b</i><sub>2</sub><i>b</i><sub>3</sub>...] </div><p>This is equivalent to find the eigenvalues and eigenvectors of <span class=formula><i>Y</i><sup><i>T</i></sup><i>Y</i></span>, the correlation matrix of the observed data. The multidimensional (or multivariate, in statistical jargon) probability density function of Y is written:</p><div class=formula><i>p</i>(<i>Y</i>) ~ <i>exp</i>( − <i>r</i><sup><i>T</i></sup><i>M</i><i>r</i>) </div><p>where <cite>r</cite> is the position is the <cite>(a,b)</cite> observation space, and <cite>M</cite> the correlation matrix. Diagonalizing the matrix <cite>M</cite> corresponds to finding a rotation matrix <cite>U</cite> such that:</p><div class=formula><i>p</i>(<i>Y</i>) ~ <i>exp</i>( − <i>r</i><sup><i>T</i></sup><i>U</i><sup><i>T</i></sup><i>S</i><i>U</i><i>r</i>) </div><p>With <cite>S</cite> a diagonal matrix. In other words, <cite>U</cite> is a rotation of the observation space to change to a basis where the probability density function is written:</p><div class=formula><i>p</i>(<i>Y</i>) ~ <i>exp</i>( − <span class=limits><sup class=limit> </sup><span class=limit><span class=symbol>∑</span></span><sub class=limit><i>i</i></sub></span><i>σ</i><sub><i>i</i></sub><i>r</i><span class=scripts><sup class=script>2</sup><sub class=script><i>i</i></sub></span>) = <span class=limits><sup class=limit> </sup><span class=limit><span class=symbol>∏</span></span><sub class=limit><i>i</i></sub></span><i>exp</i>( − <i>σ</i><sub><i>i</i></sub><i>r</i><span class=scripts><sup class=script>2</sup><sub class=script><i>i</i></sub></span>) </div><p>In this new basis, <cite>Y</cite> can thus be interpreted as a sum of independent normal processes of different variance.</p><p>We can thus picture the PCA as a way of finding independent normal processes. The different steps of the argument exposed above can be pictured in the following figure:</p><div class="figure align-center"><img alt src=../science/attachments/ica_pca/pca_on_gaussian_data.png style="width: 80%;"><p class=caption>First we represent samples drawn from <cite>x</cite> and <cite>y</cite> in their original space, the basis of the independent variables. Then we represent the (<cite>a</cite>, <cite>b</cite>) samples, and we apply PCA on these samples, to estimate the eigenvectors of the covariance matrix. Then we represent the data projected in the basis estimated by PCA. One important detail to note, is that after PCA, the data is most often rescaled: each direction is divided by the corresponding sample standard deviation identified by PCA. After this operation, all directions of space play the same role, the data is spheric, or “white”.</p></div><p>PCA was able to identify the original independent variables <cite>x</cite> and <cite>y</cite> in the <cite>a</cite> and <cite>b</cite> samples only because they were mixed with different variance. For a isotropic Gaussian model, any basis can describe the data in terms of independent normal process.</p></div><div class=section id=pca-on-non-normal-data><h2>PCA on non normal data</h2><p>More generally, the PCA algorithm can be understood as an algorithm finding the direction of space with the highest sample variance, and moving on to the orthogonal subspace of this direction to find the next highest variance, and iteratively discovering an ordered orthogonal basis of highest variance. This is well adapted to normal processes, as their covariance is indeed diagonal in an orthogonal basis. In addition, the resulting vectors come with a “PCA score”, ie the variance of the data projected along the direction they define. Thus when using PCA for dimension reduction, we can choose the subspace defined by the first <cite>n</cite> PCA vectors, on the basis that they explain a given percentage of the variance, and that the subspace they define is the subspace of dimension <cite>n</cite> that explains the largest possible fraction of the total variance.</p><p>However, on strongly non-Gaussian processes, the variance may not be the quantity of interest.</p><p>Let us consider the same model as above, with two independent variables <cite>x</cite> and <cite>y</cite> thought with strongly non-Gaussian distributions. Here we use a mixture of a narrow Gaussian, and wide one, to populate the tails:</p><img alt class=align-center src=../science/attachments/ica_pca/non_gaussian_pdf.png style="width: 40%;"><p>We can apply the same operations on these random variables: change of basis to an observation basis made of <cite>a</cite> and <cite>b</cite>, and PCA on the resulting sample:</p><img alt class=align-center src=../science/attachments/ica_pca/pca_on_non_gaussian_data.png style="width: 80%;"><p>We can see that the PCA did not properly identify the original independent variables. The variance criteria is not good-enough when the principle axis of the observed distribution are not orthogonal, as the highest variance can be found in a direction mixing the two process. Indeed the largest PCA direction is found slightly off axis. In addition the second direction can only be found orthogonal to the first one, as this is a restriction of PCA.</p><p>On the other side, the data after PCA is much more spheric than the original data. No strong anisotropy is found in the central part of the sample cloud, which contributes most to the variance.</p></div><div class=section id=ica-independent-non-gaussian-variables><h2>ICA: independent, non-Gaussian variables</h2><p>For strongly non-Gaussian processes, the above example shows that separating independent process should be done by looking at fine details of the distribution, such as the tails. Indeed, after PCA, the Gaussian part of the processes have been separated by their variance, and the resulting, rescaled, samples cannot be decomposed in independent process in a Gaussian model, as they all have the same variance, and would already be considered independent under a Gaussian hypothesis.</p><p>A popular class of algorithms to separate independent sources, called ICA (independent component analysis) makes the simplification that finding independent sources out of such data can be reduced to finding maximally non-Gaussian. Indeed, the central-limit theorem tells us that the sum of non-Gaussian processes lead to Gaussian process. Conversely, with equal variance multivariate samples, the more non-Gaussian a signal extracted from the data, the less independent -and non-Gaussian- variables it contains.</p><p>A good discussion of these arguments can be found in following paper: <a class="reference external" href=http://www.cis.hut.fi/aapo/papers/IJCNN99_tutorialweb/IJCNN99_tutorial3.html>http://www.cis.hut.fi/aapo/papers/IJCNN99_tutorialweb/IJCNN99_tutorial3.html</a></p><p>ICA is thus an optimization algorithm that from the data extracts the direction with the least-Gaussian PDF, removes the data explained by this variable from the signal, and iterates.</p><p>Applying ICA to the previous model yields the following:</p><img alt class=align-center src=../science/attachments/ica_pca/ica_on_non_gaussian_data.png style="width: 80%;"><p>We can see that ICA has well identified the original independent data variables. Its use of the tails of the distribution was paramount for this task. In addition, ICA relaxes the constraint that all identified directions must be perpendicular. This flexibility was also important to match our data.</p><div class=note><p class="first admonition-title">Note</p><p class=last>This discussion can now be seen as an <a class="reference external" href=http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html>example of the scikit-learn</a>. Thus you can replicate the figure using the code in the scikit.</p></div></div><div class=hr style="margin-bottom: -.5em;"></div><span class=social_links><a href=http://twitter.com/share class=twitter-share-button data-count=horizontal data-via=GaelVaroquaux>Tweet</a><script type=text/javascript src=http://platform.twitter.com/widgets.js async defer></script><span class=g-plusone data-size=medium></span></span><a href=# class=go-top>Go Top</a><div class=comments><div id=disqus_thread></div><script type=text/javascript>
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = "gaelvaroquaux"; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href=http://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=http://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><footer class=footer><p>&copy; Gaël Varoquaux &ndash; Built with <a href=https://github.com/PurePelicanTheme/pure>Pure Theme</a> for <a href=http://blog.getpelican.com/>Pelican</a></p></footer></div></div></div><script src=//cdnjs.cloudflare.com/ajax/libs/fitvids/1.0.1/jquery.fitvids.min.js></script><script>
        $(document).ready(function(){
            $(".content").fitVids();
        });
    </script><script>
        var $top = $('.go-top');

        // Show or hide the sticky footer button
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                $top.fadeIn(200);
            } else {
                $top.fadeOut(200);
            }
        });

        // Animate the scroll to top
        $top.click(function(event) {
            event.preventDefault();
            $('html, body').animate({scrollTop: 0}, 300);
        })

        // Makes sure that the href="#" attached to the <a> elements
        // don't scroll you back up the page.
        $('body').on('click', 'a[href="#"]', function(event) {
            event.preventDefault();
        });
    </script><script src=https://apis.google.com/js/platform.js async defer></script><script type=text/javascript>
        var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
        document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script><script type=text/javascript>
        try {
            var pageTracker = _gat._getTracker("UA-55589822-1");
            pageTracker._trackPageview();
            } catch(err) {}
    </script></body></html>