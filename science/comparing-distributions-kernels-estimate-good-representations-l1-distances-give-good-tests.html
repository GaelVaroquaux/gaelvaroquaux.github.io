<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Gaël Varoquaux, computer / data / health science">

        <link rel="alternate"  href="http://gael-varoquaux.info/feeds/all.atom.xml" type="application/atom+xml" title="Gaël Varoquaux Full Atom Feed"/>

        <title>Comparing distributions: Kernels estimate good representations, l1 distances give good tests -- Gaël Varoquaux: computer / data / health science</title>

    <link href="https://mastodon.social/@GaelVaroquaux" rel="me">

    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.3.0/pure-min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css">
    <link rel="stylesheet" href="http://gael-varoquaux.info/theme/css/pure.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
</head>

<body>
<div class="pure-g-r" id="layout">
    <div class="sidebar sidebar-article pure-u">
        <header class="header-article">
            <hgroup>
                <a href="http://gael-varoquaux.info">
		    <img class="avatar" alt="Gaël Varoquaux" 
                     src="https://gael-varoquaux.info/images/gael.png">
                </a>
                <a href="http://gael-varoquaux.info" class="article-info">
		    <h2 class="article-info">Meyer Scetbon & Gaël Varoquaux</h2>
		</a>
                <p>Sun 08 December 2019</p>
                <a href="/">&larr;Home</a>
            </hgroup>
        </header>
    </div>
    <div class="pure-u">
        <div class="content">
            <section class="post">
                <header class="post-header">
                    <h1>Comparing distributions: Kernels estimate good representations, l1 distances give good tests</h1>
                        <p class="post-meta">
                            under                                 <a class="post-category" href="http://gael-varoquaux.info/tag/science.html">science</a>
                                <a class="post-category" href="http://gael-varoquaux.info/tag/research.html">research</a>
                                <a class="post-category" href="http://gael-varoquaux.info/tag/machine-learning.html">machine learning</a>
                                <a class="post-category" href="http://gael-varoquaux.info/tag/two-sample-testing.html">two-sample testing</a>
                                <a class="post-category" href="http://gael-varoquaux.info/tag/conferences.html">conferences</a>
                                <a class="post-category" href="http://gael-varoquaux.info/tag/statistics.html">statistics</a>
		    <span class="readtime">
			&nbsp Read time: 7 min.
		    </span>
<!--<script src="https://apis.google.com/js/platform.js" async defer></script>-->
<span class="social_links">
<a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="GaelVaroquaux">Tweet</a><script type="text/javascript" src="http://platform.twitter.com/widgets.js" async defer></script>
<!-- Place this tag where you want the +1 button to render. -->
<span class="g-plusone" data-size="medium"></span>
</span>                        </p>
                </header>
            </section>
            <div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Given two set of observations, are they drawn from the same
distribution? Our paper <a class="reference external" href="https://papers.nips.cc/paper/9398-comparing-distributions-ell_1-geometry-improves-kernel-two-sample-testing.html">Comparing distributions: l1 geometry
improves kernel two-sample testing</a>
at the <strong>NeurIPS 2019 conference</strong> revisits this classic statistical
problem known as “two-sample testing”.</p>
<p class="last">This post explains the context and the paper with a bit of hand
waiving.</p>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#the-context-two-sample-testing" id="id1">The context: two-sample testing</a></li>
<li><a class="reference internal" href="#from-kernel-mean-embeddings-to-distances-on-distributions" id="id2">From kernel mean embeddings to distances on distributions</a></li>
<li><a class="reference internal" href="#controlling-the-weak-convergence-of-probability-measures" id="id3">Controlling the weak convergence of probability measures</a></li>
<li><a class="reference internal" href="#two-sample-testing-procedures" id="id4">Two-sample testing procedures</a></li>
<li><a class="reference internal" href="#the-l1-metric-provides-best-testing-power" id="id5">The L1 metric provides best testing power</a></li>
</ul>
</div>
<div class="section" id="the-context-two-sample-testing">
<h2><a class="toc-backref" href="#id1">The context: two-sample testing</a></h2>
<p>Given two samples from two unknown populations, the goal of two-sample tests is
to determine whether the underlying populations differ with a statistical
significance. For instance, we may care to know whether the
McDonald’s and KFC use different logic to chose locations of restaurants
across the US. This is a difficult question: we have access to data points,
but not the underlying generative mechanism, that is probably governed by
marketing strategies.</p>
<img alt="" class="align-center" src="attachments/comparing_distributions_l1/map_KFC_McDo_simple.png" style="width: 70%;" />
</div>
<div class="section" id="from-kernel-mean-embeddings-to-distances-on-distributions">
<h2><a class="toc-backref" href="#id2">From kernel mean embeddings to distances on distributions</a></h2>
<p>In the example of spatial distributions restaurants,
there is <strong>a lot of information in how close observed data
points lie in the original measurement space (here geographic coordinates)</strong>.
Kernel methods arise naturally to capture this information. They can be
applied to distributions, building representatives of distributions:
<a class="reference external" href="https://en.wikipedia.org/wiki/Kernel_embedding_of_distributions">Kernel embeddings of distributions</a>. The
mean embedding of a distribution P with a kernel k is written:</p>
<div class="formula">
<i>μ</i><sub><i>P</i></sub>(<i>t</i>) :  = <span class="limits"><span class="limit">⌠</span><span class="limit">⌡</span></span><sub>ℝ<sup><i>d</i></sup></sub><i>k</i>(<i>x</i>, <i>t</i>)<i>dP</i>(<i>x</i>)
</div>
<p>Intuitively, it is related to <a class="reference external" href="https://en.wikipedia.org/wiki/Kernel_density_estimation">Kernel Density Estimates (KDEs)</a> which
estimate a density in continuous space by smoothing the observed data
points with a kernel.</p>
<div class="figure align-right">
<img alt="" src="attachments/comparing_distributions_l1/kde.jpg" />
<p class="caption">Kernel mean embeddings for two distributions of points</p>
</div>
<p>For two-sample testing, kernel embeddings can be used to compute distances
between distributions, building metrics over the space of probability
measures. Metrics between probability measures can be defined via the
notion of Integral Probability Metric (IPM): as a difference of
expectations:</p>
<div class="formula">
<span class="text">IPM</span>[<i>F</i>, <i>P</i>, <i>Q</i>] :  = sup<sub><i>f</i> ∈ <i>F</i></sub>(𝔼<sub><i>x</i> ~ <i>P</i></sub><span class="symbol">[</span><i>f</i>(<i>x</i>)<span class="symbol">]</span> − 𝔼<sub><i>y</i> ~ <i>Q</i></sub><span class="symbol">[</span><i>f</i>(<i>y</i>)<span class="symbol">]</span>)
</div>
<p>where F is a class of functions. This definition is appealing because it
<strong>characterizes the difference between P and Q by the function for which
the expectancy differs most</strong>. The specific choice of class of function
defines the metric. If we now consider a kernel, it implicitly defines a
space of functions (intuitively related to all the possible KDEs
generated by varying data points): a Reproducible Kernel Hilbert Space
(RKHS). Defining a metric (an IPM) with a function class F as the unit
ball in such an RKHS, is known as the Maximum Mean Discrepancy (MMD). It
can be shown that, rather than computing the maximum, the MMD has a more
convenient expression, the RKHS distance between the mean embeddings:</p>
<div class="formula">
<span class="text">MMD</span>[<i>P</i>, <i>Q</i>] = ∥<i>μ</i><sub><i>P</i></sub> − <i>μ</i><sub><i>Q</i></sub>∥<sub><i>H</i><sub><i>k</i></sub></sub>
</div>
<p>For good choices of kernels, the MMD has appealing mathematical
properties to compare distributions. With kernels said to be
characteristic, eg Gaussian kernels, the MMD is a metric: MMD[P, Q] = 0
if and only if P = Q. Using the MMD for two-sample testing –given only
observations from the distributions, and not P and Q–  requires using an
empirical estimation of the MMD. This can be done by computing the RKHS
norm in the expression above, which leads to summing kernel evaluations
on all data points in P and Q.</p>
<p>Our work builds upon this framework, but deviates a bit from the
classical definition of MMD as it addresses the question of which norm is
best to use on the difference of mean embeddings, µQ - µP (as well as
other representatives, namely the smooth characteristic function, SCF).
We consider a wider family of metrics based on the Lp distances between
mean emdeddings (p=2 recovers the classic framework):</p>
<div class="formula">
<i>d</i><sub><i>L</i><sup><i>p</i></sup>, <i>μ</i></sub>(<i>P</i>, <i>Q</i>) :  = <span class="symbol">(</span><span class="limits"><sup class="limit"> </sup><span class="limit">⌠</span><span class="limit">⌡</span><sub class="limit"><i>t</i> ∈ ℝ<sup><i>d</i></sup></sub></span>|<i>μ</i><sub><i>P</i></sub>(<i>t</i>) − <i>μ</i><sub><i>Q</i></sub>(<i>t</i>)|<sup><i>p</i></sup><i>d</i>Γ(<i>t</i>)<span class="symbol">)</span><sup>1 ⁄ <i>p</i></sup>
</div>
<p>where Γ is a Borel probability measure absolutely continuous.</p>
</div>
<div class="section" id="controlling-the-weak-convergence-of-probability-measures">
<h2><a class="toc-backref" href="#id3">Controlling the weak convergence of probability measures</a></h2>
<p>We show that these metrics have good properties. Specifically, for p ≥ 1,
as soon as the kernel is bounded continuous and characteristic, these
metrics metrize the weak convergence. What this means is that these
metrics tend to zero if and only if P and Q weakly converge.</p>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Convergence_of_measures#Weak_convergence_of_measures">weak convergence of probability measures</a>
is a notion of convergence that is based <strong>not just on having events with
probabilities that are the same for the two distributions, but also that some events are
“close”</strong>. Indeed, classic convergence in probability just tells us that
the same observation should have the same probability in the two distributions. Weak convergence takes in account the topology of the
observations. For instance, to go back to the problem of spatial
distributions of restaurants, it does not only look at whether the
probabilities of having a Mc Donald’s or a KFC restaurant converge on
11th Wall Street, but also at restaurants are likely on 9th Wall Street.</p>
<p>A simple example to see why these matters is to consider two Dirac
distributions: spikes in a single point. If we bring these spikes closer
and closer, merely looking at the probability of events in the same exact
position will not detect any convergence until the spikes exactly
overlap.</p>
<p>Using kernel embeddings of distributions enables to capture the aspects
of convergence in the spatial domain because the kernels used give a
spatial smoothness to the representatives:</p>
<img alt="" class="align-center" src="attachments/comparing_distributions_l1/converging_diracs.png" style="width: 70%;" />
<p>Having a metric on probability distributions that captures the topology
of the observations is important for many applications, for instance when
fitting GANs to generate images: the goal is not to only capture that
images are exactly the same, but also that they maybe be “close”.</p>
</div>
<div class="section" id="two-sample-testing-procedures">
<h2><a class="toc-backref" href="#id4">Two-sample testing procedures</a></h2>
<p>Now that we have built metrics, we can derive two-sample test statistics.
A straightforward way of doing it would involve large sums on all the
observations, which would be costly. Hence, we resort to a good
approximation by sampling a set of {Tj} locations from the distribution
Γ:</p>
<div class="formula">
<i>d̂</i><span class="scripts"><sub class="script">ℓ<sub><i>p</i></sub>, <i>μ</i>, <i>J</i></sub><sup class="script"><i>p</i></sup></span>[<i>X</i>, <i>Y</i>] :  = <i>n</i><sup><i>p</i> ⁄ 2</sup><span class="limits"><sup class="limit"> </sup><span class="limit">⎲</span><span class="limit">⎳</span><sub class="limit"><i>j</i> = 1..<i>J</i></sub></span>|<i>μ</i><sub><i>X</i></sub>(<i>T</i><sub><i>j</i></sub>) − <i>μ</i><sub><i>Y</i></sub>(<i>T</i><sub><i>j</i></sub>)|<sup><i>p</i></sup>
</div>
<p>We show that this approximation maintains (almost surely) the appealing
metric properties, generalizing the results that were established by
<a class="reference external" href="http://papers.nips.cc/paper/5685-fast-two-sample-testing-with-analytic-representations-of-probability-measures">Chwialkowski et al 2015</a>
for the special case of the L2 metric.</p>
<div class="figure align-center">
<img alt="" src="attachments/comparing_distributions_l1/optimizing_position.png" style="width: 70%;" />
<p class="caption">Sampling at different positions</p>
</div>
<p>We further develop the testing procedures by showing that other tricks
known to improve testing with the L2 metric can be adapted to other
metrics, such as the L1 metric. Fast and performant tests can be obtained
by optimizing the test locations –using an upper-bound on the test power–
or by testing in the Fourrier domain, using the Smooth Characteristic
Function of the kernel. Even in the case of the L1 metric, the null
distribution of the test statistic can be derived, leading to tests that
can control errors without permutations.</p>
</div>
<div class="section" id="the-l1-metric-provides-best-testing-power">
<h2><a class="toc-backref" href="#id5">The L1 metric provides best testing power</a></h2>
<p>Going back to our question of which norm on the difference of
distribution representative is best suited to detect, we show that when
using analytics kernels, such as the Gaussian kernel, the L1 metric
improves upon the L2 metric, which corresponds to the classic definition
of the MMD.</p>
<p>Indeed, analytic kernels are non-zero almost everywhere. As a result,
when P is different from Q, the difference between their mean embeddings
will be dense, as well as the differences between the representatives
that we use to build our tests (for instance the values at the locations
that we use to build the tests above). l1 norms capture better dense
differences than l2 norms –this is the reason why, used as penalties,
they induce sparsity.</p>
<img alt="" class="align-right" src="attachments/comparing_distributions_l1/l1_vs_l2.png" style="width: 150px;" />
<p>A simple intuition is that dense vectors tend to lie in the diagonals of
the measurement basis, as none of their coordinates are zero. On these
diagonals, the l1 norm is much larger than the l1 norm of vectors with
some zero, or nearly-zero coordinates.</p>
<div class="topic">
<p class="topic-title"><strong>Summary</strong></p>
<p>For a very simple summary, the story is that: to perform tests of
whether two distributions differs, it is useful to compute a “mean
Kernel embedding” –similar to a Kernel density estimate, but without
normalization– of each distribution, and consider the l1 norm of the
difference of these embeddings. They can be computed on a small number
of locations, either drawn at random or optimized. This approach is
reminiscent of looking at the total variation between the measures,
however the fact that it uses Kernels makes it robust to small spatial
noise in the observations, unlike the total variation for which events
must perfectly coincide in both set of observations (the total
variation does not metrize the weak convergence).</p>
</div>
<div class="topic">
<p class="topic-title"><strong>References</strong></p>
<p>The framework exposed here is one that was developed over a long line
of research, which our work builds upon. <a class="reference external" href="https://papers.nips.cc/paper/9398-comparing-distributions-ell_1-geometry-improves-kernel-two-sample-testing.html">Our paper</a>
gives a complete list of references, however, some useful review
papers are</p>
<ul class="simple">
<li>C.-J. Simon-Gabriel and B. Schölkopf. Kernel distribution
embeddings: Universal kernels, <em>characteristic kernels and kernel
metrics on distributions</em>, <a class="reference external" href="https://arxiv.org/abs/1604.0525">arXiv:1604.05251</a>, 2016.</li>
<li>A. Gretton, K.M. Borgwardt, M.J. Rasch, B. Schölkopf, A. Smola; <em>A
Kernel Two-Sample Test</em>, <a class="reference external" href="http://www.jmlr.org/papers/v13/gretton12a.html">JMLR, 2012</a>.</li>
<li><a class="reference external" href="https://slideslive.com/38921490/interpretable-comparison-of-distributions-and-models">The NeurIPS 2019 tutorial</a>,
by Gretton, Sutherland, and Jitkrittum, is extremely didactic and gives
a lot of big picture</li>
</ul>
</div>
<p>·</p>
</div>

            <div class="hr" style="margin-bottom: -.5em;"></div>
<!--<script src="https://apis.google.com/js/platform.js" async defer></script>-->
<span class="social_links">
<a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="GaelVaroquaux">Tweet</a><script type="text/javascript" src="http://platform.twitter.com/widgets.js" async defer></script>
<!-- Place this tag where you want the +1 button to render. -->
<span class="g-plusone" data-size="medium"></span>
</span>            <a href="#" class="go-top">Go Top</a>
<footer class="footer">
    <p>&copy; Gaël Varoquaux &ndash;
        Built with <a href="https://github.com/PurePelicanTheme/pure">Pure Theme</a>
        for <a href="http://blog.getpelican.com/">Pelican</a>
    </p>
</footer>        </div>
    </div>
</div>
    <script
src="//cdnjs.cloudflare.com/ajax/libs/fitvids/1.0.1/jquery.fitvids.min.js"></script>
    <script>
        $(document).ready(function(){
            $(".content").fitVids();
        });
    </script>
    <script>
        var $top = $('.go-top');

        // Show or hide the sticky footer button
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                $top.fadeIn(200);
            } else {
                $top.fadeOut(200);
            }
        });

        // Animate the scroll to top
        $top.click(function(event) {
            event.preventDefault();
            $('html, body').animate({scrollTop: 0}, 300);
        })

        // Makes sure that the href="#" attached to the <a> elements
        // don't scroll you back up the page.
        $('body').on('click', 'a[href="#"]', function(event) {
            event.preventDefault();
        });
    </script>
    <script  language="JavaScript" type="text/javascript">
    /* From http://evirtus.net/pub/eflickrstream.asp */
    /* Global variables */
    var bolOnLoadRun=false;
    var bolFlickrRun=false;
    var objFlickrOutput=null;
    
    /* OnLoad events */
    window.onload = function() {
    bolOnLoadRun=true;
    if (! /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent) ) {
	eFlickrStream();
    };
    }
    
    /*
    eFlickrStream - Lovely, lovely photos! :-)
    - 02.08.2007 @ 14:33: Initial version
    */
    function eFlickrStream() {
    if (!bolFlickrRun ) { return false; } else { bolFlickrRun=false; }
    if ((!document.createElement) && (!document.getElementById) && (!document.getElementsByTagName) ) { return false; }// Test for required browser capabilities
    
    /*   *** Options/start ***   */
    var strTarget='flickrstream';// Target container for the flickr badge (must allready exist in the document)
    /*   *** Options/end ***     */
    
    if (!document.getElementById(strTarget)){return false;}// Exsist if target doesn't exsist
    
    var objTarget=document.getElementById(strTarget);
    objTarget.appendChild(objFlickrOutput);// Append output
    
    objFlickrOutput=null;// We're done, delete stored variable value
    objFlickr=null;
    }
    
    /*
    jsonFlickrFeed - Triggered when the Flickr json "feed" is loaded, generates Flickr photos output list item (Deefer "trigger method" from Patrick Quinn-Graham's DOM Flickr Badge)
    - 02.08.2007 @ 14:33: Initial version
    - 23.11.2007 @ 08:24: Updated to use eSetAttr(), enables the flickr link in IE
    - 29.11.2007 @ 05:52: Some changes for easier configuration (of both options and text-strings)
    */
    function jsonFlickrFeed(objFlickr) {
    if ((!document.createElement) && (!document.getElementById) && (!document.getElementsByTagName) ) { return false; }// Test for required browser capabilities
    
    /*   *** Options/start ***   */
    var intMaxImages=10;// Maximum number of images to show (1-20)
    var bolThumbSquare=true;// Load the square thumbnails
    var intThumbWidth=75;// Thumbnail width (omitted if set to "0")
    var strLightbox='flickrstream';// Lightbox "group name" (define to mark the thumbnails for lightbox usage, adds rel="lightbox[value]" to image links)
    var strImgAlt='[title]';// Alternative title for images ("[title]" will be replaced by image title)
    var strImgLinkTitle='View a larger version of \"[title]\"';// Link title for the image link
    /*   *** Options/end ***     */
    
    var objElm,objTxt,objImg,objLnk,objTmp,strTmp;
    var intPhotos=objFlickr.items.length;
    var objOut=document.createElement('ul');
    objOut.setAttribute('class','flickrlist');
    
    for (var iCntA=0; ( (iCntA<intPhotos) && (iCntA<intMaxImages)); iCntA++) {
    
	objElm=document.createElement('li');// Create item container element (<li>)
    
	objLnk=document.createElement('a');// Create link (to the large(r) photo)
	objLnk.setAttribute('href',objFlickr.items[iCntA].link);
	objLnk.setAttribute('title',strImgLinkTitle.replace('[title]',objFlickr.items[iCntA].title));
	if (strLightbox!=''){objLnk.setAttribute('rel','lightbox['+strLightbox+']');}// Add relation value for lightbox usage
    
	strTmp=objFlickr.items[iCntA].media.m;// Retreive thumbnail URI
	if (bolThumbSquare){strTmp=strTmp.replace('_m.jpg','_s.jpg');}// Swap default thumbnail for square...
    
	objImg=document.createElement('img');// Main thumbnail
	eSetAttr(objImg,'class','photo');
	objImg.setAttribute('src',strTmp);
	objImg.setAttribute('alt',strImgAlt.replace('[title]',objFlickr.items[iCntA].title));
	if (intThumbWidth!=0){objImg.setAttribute('width',intThumbWidth);}
    
	objLnk.appendChild(objImg);// Append thumbnail to link
	objElm.appendChild(objLnk);// Append link to container (li)

	objOut.appendChild(objElm);// Append container to output object
	}
    
    objFlickrOutput=objOut;// Store output for later use...
    
    if (bolOnLoadRun) {
	eFlickrStream();
	}
    else {
	bolFlickrRun=true;
	}
    }
    
    /*
    eSetAttr/eGetAttr - An attempt to tame IE's weird DOM model
    - 22.11.2007 @ 12:50 - Initial version
    */
    function eSetAttr(objTarget,strAttr,strValue) {
    if (typeof window.attachEvent!='undefined' && typeof window.opera=='undefined') {
    //    //Set attributes for IE
	switch (strAttr) {
	    case('class'):
		objTarget.setAttribute('className',strValue);
		break;
	    case('style'):
		objTarget.style.cssText=strValue;
		break;
	    default:
		objTarget.setAttribute(strAttr,strValue);
		break;
	    }
	}
    else {
    //    //Set attributes for /Other browsers/
	objTarget.setAttribute(strAttr,strValue);
	}
    //66885349@N03
    }
    </script>
    <script type="text/javascript" src="https://api.flickr.com/services/feeds/photos_public.gne?id=66885349@N03&amp;format=json&amp;" defer="defer"></script>
</body>
</html>