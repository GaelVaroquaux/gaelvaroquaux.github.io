<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Gaël Varoquaux, computer / data / health science">

        <link rel="alternate"  href="http://gael-varoquaux.info/feeds/all.atom.xml" type="application/atom+xml" title="Gaël Varoquaux Full Atom Feed"/>

        <title>2019: my scientific year in review -- Gaël Varoquaux: computer / data / health science</title>


    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.3.0/pure-min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.2.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="http://gael-varoquaux.info/theme/css/pure.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
</head>

<body>
<div class="pure-g-r" id="layout">
    <div class="sidebar sidebar-article pure-u">
        <header class="header-article">
            <hgroup>
                <a href="http://gael-varoquaux.info">
		    <img class="avatar" alt="Gaël Varoquaux" 
                     src="http://gael-varoquaux.info/images/gael.png">
                </a>
                <a href="http://gael-varoquaux.info" class="article-info">
		    <h2 class="article-info">Gaël Varoquaux</h2>
		</a>
                <p>Sun 05 January 2020</p>
                <a href="/">&larr;Home</a>
            </hgroup>
        </header>
    </div>
    <div class="pure-u">
        <div class="content">
            <section class="post">
                <header class="post-header">
                    <h1>2019: my scientific year in review</h1>
                        <p class="post-meta">
                            under                                 <a class="post-category" href="http://gael-varoquaux.info/tag/science.html">science</a>
                                <a class="post-category" href="http://gael-varoquaux.info/tag/research.html">research</a>
                                <a class="post-category" href="http://gael-varoquaux.info/tag/machine-learning.html">machine learning</a>
                                <a class="post-category" href="http://gael-varoquaux.info/tag/neuroimaging.html">neuroimaging</a>
                                <a class="post-category" href="http://gael-varoquaux.info/tag/statistics.html">statistics</a>
                                <a class="post-category" href="http://gael-varoquaux.info/tag/yearly-report.html">yearly report</a>
		    <span class="readtime">
			&nbsp Read time: 9 min.
		    </span>
<!--<script src="https://apis.google.com/js/platform.js" async defer></script>-->
<span class="social_links">
<a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="GaelVaroquaux">Tweet</a><script type="text/javascript" src="http://platform.twitter.com/widgets.js" async defer></script>
<!-- Place this tag where you want the +1 button to render. -->
<span class="g-plusone" data-size="medium"></span>
</span>                        </p>
                </header>
            </section>
            <p>My current research spans wide: from brain sciences to core data
science. My overall interest is to build <strong>methodology drawing insights from
data</strong> for questions that have often been addressed qualitatively. If I can
highlight a few publications from 2019 <a class="footnote-reference" href="#id2" id="id1">[1]</a>, the common thread would be
computational statistics, from dirty data to brain images. Let me try to
give the gist of these progresses, in simple terms.</p>
<table class="side-hanging docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td>It’s already 2020, I’m always late.</td></tr>
</tbody>
</table>
<div class="contents topic" id="highlights">
<p class="topic-title">Highlights</p>
<ul class="simple">
<li><a class="reference internal" href="#comparing-distributions" id="id5">Comparing distributions</a></li>
<li><a class="reference internal" href="#predictive-pipelines-on-brain-functional-connectomes" id="id6">Predictive pipelines on brain functional connectomes</a></li>
<li><a class="reference internal" href="#population-shrinkage-of-covariance" id="id7">Population shrinkage of covariance</a></li>
<li><a class="reference internal" href="#deep-learning-on-non-translation-invariant-images" id="id8">Deep learning on non-translation-invariant images</a></li>
<li><a class="reference internal" href="#open-science" id="id9">Open science</a></li>
</ul>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="section" id="comparing-distributions">
<h2><a class="toc-backref" href="#id5">Comparing distributions</a></h2>
<p class="align-right"><em>Fundamental computational-statistics work</em></p>
<p>What if you are given two set of observations and need to conclude on
whether they are drawn from the same distribution? We are interested in
this question for the <a class="reference external" href="https://project.inria.fr/dirtydata/">DirtyData</a>
research project, to facilitate analysis of data without manual curation.
Comparing distributions is indeed important to detect drifts in the data,
to match information across datasets, or to compensate for dataset
biases.</p>
<p>Formally, we are given two cloud of points (circle and crosses in the
figure below) and we want to develop a statistical test of whether the
distributions differ. There is an abundant literature on this topic, that I
cover in <a class="reference external" href="http://gael-varoquaux.info/science/comparing-distributions-kernels-estimate-good-representations-l1-distances-give-good-tests.html">a more detailed post on this subject</a>.
Specifically, when the observations have a natural similarity, for
instance when they live in a vector space, kernel methods are interesting
because they enable to estimate a representant of the underlying
distribution that interpolates between observations, as with <a class="reference external" href="https://en.wikipedia.org/wiki/Kernel_density_estimation">a kernel
density estimator</a>.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="http://papers.nips.cc/paper/9398-comparing-distributions-ell_1-geometry-improves-kernel-two-sample-testing"><img alt="" src="../science/attachments/comparing_distributions_l1/optimizing_position.png" style="width: 500px;" /></a>
<p class="caption">Two cloud of points, the corresponding distribution representants μ_P
and μ_Q (blue and orange), the difference between these
(black), and locations to measure this difference (red triangles).</p>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>With Meyer Scetbon, in
<a class="reference external" href="http://papers.nips.cc/paper/9398-comparing-distributions-ell_1-geometry-improves-kernel-two-sample-testing">Scetbon &amp; Varoquaux, NeurIPS</a>,
we investigate how to measure best the difference between these
representants. We show that the best choice is to take the absolute value
of the difference (the l1 norm), while the default choice had so far been
the Euclidean (l2) norm. In a nutshell, the reason is that the difference
most like is dense when the distributions differ: zero almost nowhere.</p>
<p>We were able to show that the <a class="reference external" href="https://slideslive.com/38921490/interpretable-comparison-of-distributions-and-models">sophisticated framework</a>
for efficient and powerful tests in the
Euclidean case carries over to the l1 case. In particular, our paper
gives efficient testing procedures using a small number of locations to
avoid costly computation (the red triangles in the figure above), that
can either be sampled at random or optimized.</p>
<p>My hunch is that the result is quite general: the l1 geometry is better
than the l2 one on representants of distributions. There might be more
fundamental mathematical properties behind this. The drawback is that the
l1 norm is non smooth which can be challenging in optimization settings.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="predictive-pipelines-on-brain-functional-connectomes">
<h2><a class="toc-backref" href="#id6">Predictive pipelines on brain functional connectomes</a></h2>
<p class="align-right"><em>Brain-imaging methods</em></p>
<p>Brain functional connectivity is increasingly used to extract biomarkers
of behavior and mental health. The long-term stakes are to ground
assessment of psychological traits on quantitative brain
data, rather than qualitative behavioral observations. But, to build
biomarkers, there are many details that go in estimating functional
connectivity from fMRI, something that I have studied for more than 10
years. With Kamalakar Dadi, in <a class="reference external" href="https://www.sciencedirect.com/science/article/abs/pii/S1053811919301594">Dadi et al</a>,
we ran thorough empirical benchmarks to find which methodological choices
for the various steps of the pipeline give best prediction across
multiple cohorts. Specifically, we studied 1) defining regions of
interest for signal extraction, 2) building a functional-connectivity
matrix across these regions, 3) prediction across subjects with
supervised learning on these features.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="https://www.sciencedirect.com/science/article/abs/pii/S1053811919301594"><img alt="" src="../science/attachments/2019_highlights/dadi_2019_highlights.png" style="width: 600px;" /></a>
<p class="caption">Summarizing our benchmark results.</p>
</div>
<div class="sidebar">
<p class="first sidebar-title">Recommendations</p>
<ul class="last simple">
<li>functional regions (eg from dictionary learning)</li>
<li>tangent-space for covariances</li>
<li>l2-logistic regression</li>
</ul>
</div>
<p>Results show the importance of defining regions from functional data,
ideally with a linear-decomposition method that produces soft
parcellations such as ICA or dictionary learning. To represent
connectivity between regions, the best choice is tangent-space
parametrization, a method to build a vector-space from covariance
matrices (more below). Finally, for supervised learning, a simple
l2-penalized logistic regression is the best option. With the huge popularity
of deep learning, it may surprise that linear models are the best
performer, but this is well explained by the amount of data at hand: a
cohort is typically less than 1000 individuals, which is way below the
data sizes needed to see the benefits of non-linear models.</p>
<p>A recent preprint, <a class="reference external" href="https://www.biorxiv.org/content/10.1101/741595v2.abstract">Pervaiz et al</a> from
Oxford, overall
confirms our findings, even though they investigated slightly
different methodological choices. In particular, they find tangent space
clearly useful.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>In my eyes, such benchmarking studies are important not only to improve
prediction, but also to reduce analytic variability that opens the door
to inflation of reported effects. Indeed, given 1000 individuals, the
measure of prediction accuracy of a pipeline is quite imprecise
(<a class="reference external" href="https://www.sciencedirect.com/science/article/abs/pii/S1053811917305311">Varoquaux 2018</a>).
As a consequence, trying out a bunch a analytic choices and
publishing the one that works best can lead to grossly optimistic
prediction accuracies. <strong>If we want trust in biomarkers, we need to
reduce the variability in the methods used to build them</strong>.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="population-shrinkage-of-covariance">
<h2><a class="toc-backref" href="#id7">Population shrinkage of covariance</a></h2>
<p class="align-right">Statistics for brain signals</p>
<p>Estimating covariances is central for functional brain connectivity and
in many other applications. With Mehdi Rahim, in <a class="reference external" href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518301014">Rahim et al</a>
we considered the case of a population of random processes with
related covariances, as for instance when estimating functional
connectivity from a group of individuals. For this, we combined two
mathematical ideas: that of using natural operations on covariance
matrices, and that of priors for mean-square estimation:</p>
<ul class="simple">
<li><strong>Tangent space</strong> Covariance matrices are positive-definite matrices,
for which standard arithmetics are not well suited <a class="footnote-reference" href="#id4" id="id3">[2]</a>: subtracting
two covariance matrices can lead to a matrix that cannot be
the covariance of a signal. However, a group of covariance matrices can
be transformed into points in a vector space for which standard
distances and arithmetics respect the structure of
covariances (for instance Euclidean distance between these points
approximate KL divergence between covariances). This is what we call
the <em>tangent space</em>.</li>
</ul>
<table class="side-hanging docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[2]</a></td><td>Technically, covariance matrices live on a Riemannian manifold:
a curve surface inside <em>R^{n x n}</em> that has some metric
properties.</td></tr>
</tbody>
</table>
<ul class="simple">
<li><strong>James-Stein shrinkage</strong> To estimate the mean of <em>n</em> observations, it
is actually best not to compute the average of these, but rather to
push a bit this average toward a prior guess. The better the
guess, the more this “push” helps. The more the number of observations,
the more gentle this push should be. This strategy is known as
<a class="reference external" href="https://en.wikipedia.org/wiki/James%E2%80%93Stein_estimator">James-Stein shrinkage</a> and it
is in my opinion one of the most beautiful results in statistics.
It can be seen as a Bayesian posterior, but it comes with guarantees
that do not require the model to be true and that control estimation
error, rather than a posterior probability.</li>
</ul>
<p>James-Stein shrinkage is easily written for quadratic errors on vectors,
but cannot be easily applied to covariances, as they do not live in a vector
space and we would like to control a KL divergence rather than
a quadratic error. Our work combined both ideas to give an excellent
estimator of a family of related covariances that is also very
computationally efficient. We call it PoSCE: Population Shrinkage
Covariance Estimation.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518301014"><img alt="" src="../science/attachments/2019_highlights/posce.png" style="width: 600px;" /></a>
<p class="caption">Schema of the estimation strategy: projecting the covariances matrices
into a tangent space, shrinkage to a group mean, but taking in account
the anisotropy of the dispersion of the group, and projecting back to
covariances.</p>
</div>
<p>It is easy to see how accounting for group information in the estimation
of individual covariances can help stabilizing them. However, will it be
beneficial if we are interested in the differences between these
covariances, for instance to ground biomarkers, as studied above? Our
results show that it does indeed help building better biomarkers, for
instance to predict brain age. The larger the group of covariances used,
the larger the benefits.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518301014"><img alt="" src="../science/attachments/2019_highlights/posce_age_learning_curve.png" style="width: 500px;" /></a>
<p class="caption">Error in predicting brain aging decreases when more individuals are used
to build the biomarker.</p>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="deep-learning-on-non-translation-invariant-images">
<h2><a class="toc-backref" href="#id8">Deep learning on non-translation-invariant images</a></h2>
<p class="align-right">Computer vision</p>
<p>Brain images, in particular images of brain activity, are very different
from the natural images on which most computer-vision research focuses.
A central difference is that detecting activity in different parts of the
brain completely changes the meaning of this detection, while detecting a
cat in the left or the right of a picture on Facebook makes no
difference. This is important because many progresses of computer vision,
such as convolutional neural networks, are built on the fact that natural
images are statistically translational invariant. On the opposite, brain
images are realigned to a template, before being analyzed.</p>
<p>Convolutional architectures have been crucial to the successes of deep
learning on natural images because they impose a lot of structure on the
weights of neural networks and thus help fight estimation noise. For
predicting from brain images, the regularizations strategies that have
been successful foster spatially continuous structures. Unfortunately,
they have lead to costly non-smooth optimizations that cannot easily be
used with the optimization framework of deep learning, stochastic
gradient descent.</p>
<p>With Sergul Aydore, in <a class="reference external" href="http://proceedings.mlr.press/v97/aydore19a.html">Aydore et al, ICML</a>, we have introduced a
spatial regularization that is compatible with the deep learning toolbox.
During the stochastic optimization, we impose random spatial structure
via feature groups estimated from the data. These stabilize the input
layers of deep architecture. They also lead to iterating on smaller
representations, which greatly speeds up the algorithm.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="http://proceedings.mlr.press/v97/aydore19a.html"><img alt="" src="../science/attachments/2019_highlights/stochastic_grouping_mlp.png" style="width: 600px;" /></a>
<p class="caption">At each step of a stochastic gradient descent, we randomly pick a
feature-grouping matrix (itself estimated from the data), and use it
to reduce the data in the computations of the gradients, then invert
this reduction to update the weights.</p>
</div>
<p><a class="reference external" href="http://proceedings.mlr.press/v97/aydore19a.html">The paper</a> comes with
extensive empirical validation, including comparison to convolutional
neural networks. We benchmark the strategy on brain images, but also
on realigned faces, to show that the approach is beneficial for any
non-translational-invariant images. In particular, the approach greatly
speeds up convergence.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="http://proceedings.mlr.press/v97/aydore19a.html"><img alt="" src="../science/attachments/2019_highlights/stochastic_grouping_results.png" style="width: 600px;" /></a>
<p class="caption">Prediction accuracy as a function of training time – left: on
realigned faces – right: on brain images</p>
</div>
<p><a class="reference external" href="http://proceedings.mlr.press/v97/aydore19a.html">This paper</a> clearly
shows that <strong>one should not use convolutional neural networks on fMRI
data</strong>: these images are not translational invariant.</p>
<div class="sidebar">
<p class="first sidebar-title"><strong>Preprints</strong></p>
<p class="last">All papers are available as preprints, eg on <a class="reference external" href="http://gael-varoquaux.info/publications.html">my site</a>.</p>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
<div class="section" id="open-science">
<h2><a class="toc-backref" href="#id9">Open science</a></h2>
<p><strong>Open and reproducible science:</strong> Looking at all these publications, I
realize that every single one of them comes with code on a github
repository and is done on open data, which means that they can all be
easily reproduced. I’m very proud of the teams behind these papers.
Achieving this level of reproducibility requires hard work and
discipline. It is also a testimonial to a community investment in
software tools and infrastructure for open science that has been going on
for decades and gives the foundations on which these works build.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p><strong>A prize for scikit-learn:</strong> On this topic, a highlight of 2019 was also
that the work behind scikit-learn was acknowledged in <a class="reference external" href="../programming/getting-a-big-scientific-prize-for-open-source-software.html">an important
scientific prize</a>.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p><strong>Why open science:</strong> Why do I care so much for open science? Because in
a world of uncertainty, the claims of science must be trusted and hence
built on transparent practice (think about science and global warming).
Because it helps putting our methods in the hands of a wider public,
society at large. And because it levels the ground, making it easier for
newcomers –young scientists, or developing countries– to contribute,
which in itself makes science more efficient.</p>
</div>

            <div class="hr" style="margin-bottom: -.5em;"></div>
<!--<script src="https://apis.google.com/js/platform.js" async defer></script>-->
<span class="social_links">
<a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="GaelVaroquaux">Tweet</a><script type="text/javascript" src="http://platform.twitter.com/widgets.js" async defer></script>
<!-- Place this tag where you want the +1 button to render. -->
<span class="g-plusone" data-size="medium"></span>
</span>            <a href="#" class="go-top">Go Top</a>
<footer class="footer">
    <p>&copy; Gaël Varoquaux &ndash;
        Built with <a href="https://github.com/PurePelicanTheme/pure">Pure Theme</a>
        for <a href="http://blog.getpelican.com/">Pelican</a>
    </p>
</footer>        </div>
    </div>
</div>
    <script
src="//cdnjs.cloudflare.com/ajax/libs/fitvids/1.0.1/jquery.fitvids.min.js"></script>
    <script>
        $(document).ready(function(){
            $(".content").fitVids();
        });
    </script>
    <script>
        var $top = $('.go-top');

        // Show or hide the sticky footer button
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                $top.fadeIn(200);
            } else {
                $top.fadeOut(200);
            }
        });

        // Animate the scroll to top
        $top.click(function(event) {
            event.preventDefault();
            $('html, body').animate({scrollTop: 0}, 300);
        })

        // Makes sure that the href="#" attached to the <a> elements
        // don't scroll you back up the page.
        $('body').on('click', 'a[href="#"]', function(event) {
            event.preventDefault();
        });
    </script>
    <script  language="JavaScript" type="text/javascript">
    /* From http://evirtus.net/pub/eflickrstream.asp */
    /* Global variables */
    var bolOnLoadRun=false;
    var bolFlickrRun=false;
    var objFlickrOutput=null;
    
    /* OnLoad events */
    window.onload = function() {
    bolOnLoadRun=true;
    if (! /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent) ) {
	eFlickrStream();
    };
    }
    
    /*
    eFlickrStream - Lovely, lovely photos! :-)
    - 02.08.2007 @ 14:33: Initial version
    */
    function eFlickrStream() {
    if (!bolFlickrRun ) { return false; } else { bolFlickrRun=false; }
    if ((!document.createElement) && (!document.getElementById) && (!document.getElementsByTagName) ) { return false; }// Test for required browser capabilities
    
    /*   *** Options/start ***   */
    var strTarget='flickrstream';// Target container for the flickr badge (must allready exist in the document)
    /*   *** Options/end ***     */
    
    if (!document.getElementById(strTarget)){return false;}// Exsist if target doesn't exsist
    
    var objTarget=document.getElementById(strTarget);
    objTarget.appendChild(objFlickrOutput);// Append output
    
    objFlickrOutput=null;// We're done, delete stored variable value
    objFlickr=null;
    }
    
    /*
    jsonFlickrFeed - Triggered when the Flickr json "feed" is loaded, generates Flickr photos output list item (Deefer "trigger method" from Patrick Quinn-Graham's DOM Flickr Badge)
    - 02.08.2007 @ 14:33: Initial version
    - 23.11.2007 @ 08:24: Updated to use eSetAttr(), enables the flickr link in IE
    - 29.11.2007 @ 05:52: Some changes for easier configuration (of both options and text-strings)
    */
    function jsonFlickrFeed(objFlickr) {
    if ((!document.createElement) && (!document.getElementById) && (!document.getElementsByTagName) ) { return false; }// Test for required browser capabilities
    
    /*   *** Options/start ***   */
    var intMaxImages=10;// Maximum number of images to show (1-20)
    var bolThumbSquare=true;// Load the square thumbnails
    var intThumbWidth=75;// Thumbnail width (omitted if set to "0")
    var strLightbox='flickrstream';// Lightbox "group name" (define to mark the thumbnails for lightbox usage, adds rel="lightbox[value]" to image links)
    var strImgAlt='[title]';// Alternative title for images ("[title]" will be replaced by image title)
    var strImgLinkTitle='View a larger version of \"[title]\"';// Link title for the image link
    /*   *** Options/end ***     */
    
    var objElm,objTxt,objImg,objLnk,objTmp,strTmp;
    var intPhotos=objFlickr.items.length;
    var objOut=document.createElement('ul');
    objOut.setAttribute('class','flickrlist');
    
    for (var iCntA=0; ( (iCntA<intPhotos) && (iCntA<intMaxImages)); iCntA++) {
    
	objElm=document.createElement('li');// Create item container element (<li>)
    
	objLnk=document.createElement('a');// Create link (to the large(r) photo)
	objLnk.setAttribute('href',objFlickr.items[iCntA].link);
	objLnk.setAttribute('title',strImgLinkTitle.replace('[title]',objFlickr.items[iCntA].title));
	if (strLightbox!=''){objLnk.setAttribute('rel','lightbox['+strLightbox+']');}// Add relation value for lightbox usage
    
	strTmp=objFlickr.items[iCntA].media.m;// Retreive thumbnail URI
	if (bolThumbSquare){strTmp=strTmp.replace('_m.jpg','_s.jpg');}// Swap default thumbnail for square...
    
	objImg=document.createElement('img');// Main thumbnail
	eSetAttr(objImg,'class','photo');
	objImg.setAttribute('src',strTmp);
	objImg.setAttribute('alt',strImgAlt.replace('[title]',objFlickr.items[iCntA].title));
	if (intThumbWidth!=0){objImg.setAttribute('width',intThumbWidth);}
    
	objLnk.appendChild(objImg);// Append thumbnail to link
	objElm.appendChild(objLnk);// Append link to container (li)

	objOut.appendChild(objElm);// Append container to output object
	}
    
    objFlickrOutput=objOut;// Store output for later use...
    
    if (bolOnLoadRun) {
	eFlickrStream();
	}
    else {
	bolFlickrRun=true;
	}
    }
    
    /*
    eSetAttr/eGetAttr - An attempt to tame IE's weird DOM model
    - 22.11.2007 @ 12:50 - Initial version
    */
    function eSetAttr(objTarget,strAttr,strValue) {
    if (typeof window.attachEvent!='undefined' && typeof window.opera=='undefined') {
    //    //Set attributes for IE
	switch (strAttr) {
	    case('class'):
		objTarget.setAttribute('className',strValue);
		break;
	    case('style'):
		objTarget.style.cssText=strValue;
		break;
	    default:
		objTarget.setAttribute(strAttr,strValue);
		break;
	    }
	}
    else {
    //    //Set attributes for /Other browsers/
	objTarget.setAttribute(strAttr,strValue);
	}
    //66885349@N03
    }
    </script>
    <script type="text/javascript" src="https://api.flickr.com/services/feeds/photos_public.gne?id=66885349@N03&amp;format=json&amp;" defer="defer"></script>
</body>
</html>